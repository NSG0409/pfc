{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimateDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length=24):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Verificar se há mais de uma classe\n",
    "        unique_classes = np.unique(data[:, -1])\n",
    "        assert len(unique_classes) > 1, f\"Dataset possui apenas uma classe: {unique_classes}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx : idx + self.sequence_length, :-1]  # Features\n",
    "        y = self.data[idx + self.sequence_length - 1, -1]  # Target\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MO</th>\n",
       "      <th>DY</th>\n",
       "      <th>HR</th>\n",
       "      <th>ALLSKY_SFC_UV_INDEX</th>\n",
       "      <th>ALLSKY_SRF_ALB</th>\n",
       "      <th>CLRSKY_KT</th>\n",
       "      <th>CLOUD_AMT</th>\n",
       "      <th>T2M</th>\n",
       "      <th>PS</th>\n",
       "      <th>PW</th>\n",
       "      <th>WD10M</th>\n",
       "      <th>WD50M</th>\n",
       "      <th>WS50M</th>\n",
       "      <th>WS10M</th>\n",
       "      <th>TOA_SW_DNI</th>\n",
       "      <th>QV2M</th>\n",
       "      <th>QV10M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>32.70</td>\n",
       "      <td>18.93</td>\n",
       "      <td>93.39</td>\n",
       "      <td>2.72</td>\n",
       "      <td>140.07</td>\n",
       "      <td>139.57</td>\n",
       "      <td>6.24</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.79</td>\n",
       "      <td>13.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>24.88</td>\n",
       "      <td>18.54</td>\n",
       "      <td>93.40</td>\n",
       "      <td>2.66</td>\n",
       "      <td>138.43</td>\n",
       "      <td>137.82</td>\n",
       "      <td>6.18</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.55</td>\n",
       "      <td>13.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>51.42</td>\n",
       "      <td>18.21</td>\n",
       "      <td>93.36</td>\n",
       "      <td>2.61</td>\n",
       "      <td>138.09</td>\n",
       "      <td>137.45</td>\n",
       "      <td>6.08</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.37</td>\n",
       "      <td>13.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>66.36</td>\n",
       "      <td>17.94</td>\n",
       "      <td>93.31</td>\n",
       "      <td>2.55</td>\n",
       "      <td>137.70</td>\n",
       "      <td>136.99</td>\n",
       "      <td>5.89</td>\n",
       "      <td>3.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.18</td>\n",
       "      <td>13.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>51.16</td>\n",
       "      <td>17.69</td>\n",
       "      <td>93.27</td>\n",
       "      <td>2.50</td>\n",
       "      <td>137.93</td>\n",
       "      <td>137.20</td>\n",
       "      <td>5.62</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.06</td>\n",
       "      <td>13.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201595</th>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>19</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.67</td>\n",
       "      <td>31.79</td>\n",
       "      <td>28.00</td>\n",
       "      <td>93.33</td>\n",
       "      <td>2.72</td>\n",
       "      <td>155.45</td>\n",
       "      <td>154.80</td>\n",
       "      <td>6.44</td>\n",
       "      <td>5.66</td>\n",
       "      <td>1408.66</td>\n",
       "      <td>11.47</td>\n",
       "      <td>11.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201596</th>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.58</td>\n",
       "      <td>20.45</td>\n",
       "      <td>25.87</td>\n",
       "      <td>93.38</td>\n",
       "      <td>2.70</td>\n",
       "      <td>152.49</td>\n",
       "      <td>152.07</td>\n",
       "      <td>7.36</td>\n",
       "      <td>6.12</td>\n",
       "      <td>1408.61</td>\n",
       "      <td>11.72</td>\n",
       "      <td>11.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201597</th>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.38</td>\n",
       "      <td>31.40</td>\n",
       "      <td>23.62</td>\n",
       "      <td>93.45</td>\n",
       "      <td>2.70</td>\n",
       "      <td>148.69</td>\n",
       "      <td>148.45</td>\n",
       "      <td>7.88</td>\n",
       "      <td>6.21</td>\n",
       "      <td>1408.76</td>\n",
       "      <td>12.08</td>\n",
       "      <td>12.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201598</th>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>21.06</td>\n",
       "      <td>21.87</td>\n",
       "      <td>93.52</td>\n",
       "      <td>2.71</td>\n",
       "      <td>145.60</td>\n",
       "      <td>145.61</td>\n",
       "      <td>7.41</td>\n",
       "      <td>5.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.45</td>\n",
       "      <td>12.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201599</th>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>22.94</td>\n",
       "      <td>20.82</td>\n",
       "      <td>93.55</td>\n",
       "      <td>2.72</td>\n",
       "      <td>140.66</td>\n",
       "      <td>140.22</td>\n",
       "      <td>6.68</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.70</td>\n",
       "      <td>12.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201600 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        YEAR  MO  DY  HR  ALLSKY_SFC_UV_INDEX  ALLSKY_SRF_ALB  CLRSKY_KT  \\\n",
       "0       2001   1   1   0                 0.00         -999.00    -999.00   \n",
       "1       2001   1   1   1                 0.00         -999.00    -999.00   \n",
       "2       2001   1   1   2                 0.00         -999.00    -999.00   \n",
       "3       2001   1   1   3                 0.00         -999.00    -999.00   \n",
       "4       2001   1   1   4                 0.00         -999.00    -999.00   \n",
       "...      ...  ..  ..  ..                  ...             ...        ...   \n",
       "201595  2023  12  31  19                 2.92            0.15       0.67   \n",
       "201596  2023  12  31  20                 0.80            0.13       0.58   \n",
       "201597  2023  12  31  21                 0.05            0.16       0.38   \n",
       "201598  2023  12  31  22                 0.00         -999.00    -999.00   \n",
       "201599  2023  12  31  23                 0.00         -999.00    -999.00   \n",
       "\n",
       "        CLOUD_AMT    T2M     PS    PW   WD10M   WD50M  WS50M  WS10M  \\\n",
       "0           32.70  18.93  93.39  2.72  140.07  139.57   6.24   4.25   \n",
       "1           24.88  18.54  93.40  2.66  138.43  137.82   6.18   4.06   \n",
       "2           51.42  18.21  93.36  2.61  138.09  137.45   6.08   3.89   \n",
       "3           66.36  17.94  93.31  2.55  137.70  136.99   5.89   3.63   \n",
       "4           51.16  17.69  93.27  2.50  137.93  137.20   5.62   3.35   \n",
       "...           ...    ...    ...   ...     ...     ...    ...    ...   \n",
       "201595      31.79  28.00  93.33  2.72  155.45  154.80   6.44   5.66   \n",
       "201596      20.45  25.87  93.38  2.70  152.49  152.07   7.36   6.12   \n",
       "201597      31.40  23.62  93.45  2.70  148.69  148.45   7.88   6.21   \n",
       "201598      21.06  21.87  93.52  2.71  145.60  145.61   7.41   5.61   \n",
       "201599      22.94  20.82  93.55  2.72  140.66  140.22   6.68   4.93   \n",
       "\n",
       "        TOA_SW_DNI   QV2M  QV10M  \n",
       "0             0.00  13.79  13.73  \n",
       "1             0.00  13.55  13.49  \n",
       "2             0.00  13.37  13.31  \n",
       "3             0.00  13.18  13.18  \n",
       "4             0.00  13.06  13.06  \n",
       "...            ...    ...    ...  \n",
       "201595     1408.66  11.47  11.29  \n",
       "201596     1408.61  11.72  11.66  \n",
       "201597     1408.76  12.08  12.02  \n",
       "201598        0.00  12.45  12.39  \n",
       "201599        0.00  12.70  12.70  \n",
       "\n",
       "[201600 rows x 18 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '../../dataset2001_2024.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(-999.00, np.nan, inplace=True)\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "data['extreme_event'] = (data['T2M'] > 36).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data.iloc[:, 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normalized_data[:, :-1]  # Todas as colunas menos o target\n",
    "y = normalized_data[:, -1]   # Apenas o target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição original das classes: {np.float64(0.0): np.int64(201366), np.float64(1.0): np.int64(234)}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(f\"Distribuição original das classes: {dict(zip(unique, counts))}\")\n",
    "\n",
    "# Reamostrar usando SMOTE para lidar com desbalanceamento\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição após SMOTE: {np.float64(0.0): np.int64(201366), np.float64(1.0): np.int64(201366)}\n"
     ]
    }
   ],
   "source": [
    "# Verificar a distribuição após SMOTE\n",
    "unique_resampled, counts_resampled = np.unique(y_resampled, return_counts=True)\n",
    "print(f\"Distribuição após SMOTE: {dict(zip(unique_resampled, counts_resampled))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição no conjunto de treino: {np.float64(0.0): np.int64(161092), np.float64(1.0): np.int64(161093)}\n",
      "Distribuição no conjunto de teste: {np.float64(0.0): np.int64(40274), np.float64(1.0): np.int64(40273)}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42\n",
    ")\n",
    "\n",
    "# Verificar a distribuição após a divisão\n",
    "print(f\"Distribuição no conjunto de treino: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "print(f\"Distribuição no conjunto de teste: {dict(zip(*np.unique(y_test, return_counts=True)))}\")\n",
    "\n",
    "# Criar datasets PyTorch\n",
    "sequence_length = 24\n",
    "train_dataset = ClimateDataset(np.hstack((X_train, y_train.reshape(-1, 1))), sequence_length=sequence_length)\n",
    "test_dataset = ClimateDataset(np.hstack((X_test, y_test.reshape(-1, 1))), sequence_length=sequence_length)\n",
    "\n",
    "# Criar DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo do Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, ff_dim, num_layers, seq_length):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim * seq_length, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.flatten(1)  # Flatten para conectar à camada final\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Natan Guedes\\Documents\\Dev\\PFC\\pfc\\py9\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0194\n",
      "Epoch 2/10, Loss: 0.0064\n",
      "Epoch 3/10, Loss: 0.0055\n",
      "Epoch 4/10, Loss: 0.0045\n",
      "Epoch 5/10, Loss: 0.0039\n",
      "Epoch 6/10, Loss: 0.0035\n",
      "Epoch 7/10, Loss: 0.0035\n",
      "Epoch 8/10, Loss: 0.0034\n",
      "Epoch 9/10, Loss: 0.0033\n",
      "Epoch 10/10, Loss: 0.0029\n"
     ]
    }
   ],
   "source": [
    "# Parâmetros do modelo\n",
    "input_dim = X.shape[1]  # Número de features\n",
    "embed_dim = 16\n",
    "num_heads = 2\n",
    "ff_dim = 64\n",
    "num_layers = 2\n",
    "\n",
    "# Instanciar o modelo\n",
    "model = TransformerModel(input_dim=input_dim, embed_dim=embed_dim, num_heads=num_heads, \n",
    "                         ff_dim=ff_dim, num_layers=num_layers, seq_length=sequence_length)\n",
    "\n",
    "# Função de perda com pesos para lidar com desbalanceamento\n",
    "class_weights = torch.tensor([1.0, 10.0], dtype=torch.float32)  # Ajuste de pesos\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Função de treinamento\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[40226    40]\n",
      " [    2 40256]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     40266\n",
      "         1.0       1.00      1.00      1.00     40258\n",
      "\n",
      "    accuracy                           1.00     80524\n",
      "   macro avg       1.00      1.00      1.00     80524\n",
      "weighted avg       1.00      1.00      1.00     80524\n",
      "\n",
      "\n",
      "ROC-AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            outputs = model(x_batch).squeeze()\n",
    "            y_pred.extend(outputs.numpy())\n",
    "            y_true.extend(y_batch.numpy())\n",
    "    \n",
    "    y_pred_binary = (np.array(y_pred) > 0.5).astype(int)  # Threshold para classificação binária\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred_binary))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred_binary))\n",
    "    print(f\"\\nROC-AUC: {roc_auc_score(y_true, y_pred):.4f}\")\n",
    "\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Natan Guedes\\AppData\\Local\\Temp\\ipykernel_7180\\154195200.py:7: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  timestamps = pd.date_range(start='2000-01-01', periods=len(historical_temperatures), freq='H')  # Exemplo de timestamps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201576, 24, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4837824x1 and 16x16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 30\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_historical\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Passo 3: Ajustar timestamps para as previsões\u001b[39;00m\n\u001b[0;32m     33\u001b[0m timestamps_pred \u001b[38;5;241m=\u001b[39m timestamps[sequence_length:]  \u001b[38;5;66;03m# As previsões começam após o período inicial de entrada\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Natan Guedes\\Documents\\Dev\\PFC\\pfc\\py9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Natan Guedes\\Documents\\Dev\\PFC\\pfc\\py9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[98], line 10\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten para conectar à camada final\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Natan Guedes\\Documents\\Dev\\PFC\\pfc\\py9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Natan Guedes\\Documents\\Dev\\PFC\\pfc\\py9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Natan Guedes\\Documents\\Dev\\PFC\\pfc\\py9\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4837824x1 and 16x16)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Passo 1: Preparar os dados históricos\n",
    "historical_temperatures = data['T2M']  # Coluna do dataset original\n",
    "timestamps = pd.date_range(start='2000-01-01', periods=len(historical_temperatures), freq='H')  # Exemplo de timestamps\n",
    "historical_df = pd.DataFrame({\n",
    "    'Timestamp': timestamps,\n",
    "    'Historical': historical_temperatures\n",
    "})\n",
    "\n",
    "# Preparar as sequências históricas corretamente\n",
    "sequence_length = 24  # Número de passos no tempo usados pelo modelo\n",
    "num_features = 1  # Apenas a temperatura está sendo usada\n",
    "\n",
    "# Criar janelas deslizantes para o modelo\n",
    "X_historical = []\n",
    "for i in range(len(historical_temperatures) - sequence_length):\n",
    "    X_historical.append(historical_temperatures[i:i + sequence_length])\n",
    "\n",
    "# Transformar em tensor PyTorch com as dimensões esperadas\n",
    "X_historical = torch.tensor(X_historical, dtype=torch.float32).unsqueeze(-1)  # Shape: (n_samples, sequence_length, num_features)\n",
    "\n",
    "print(X_historical.shape)  # Deve ser algo como (n_samples, sequence_length, num_features)\n",
    "\n",
    "# Previsão com o modelo Transformer\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_historical).squeeze().numpy()\n",
    "\n",
    "# Passo 3: Ajustar timestamps para as previsões\n",
    "timestamps_pred = timestamps[sequence_length:]  # As previsões começam após o período inicial de entrada\n",
    "\n",
    "# Criar DataFrame para as previsões\n",
    "predicted_df = pd.DataFrame({\n",
    "    'Timestamp': timestamps_pred,\n",
    "    'Predicted': y_pred\n",
    "})\n",
    "\n",
    "# Passo 4: Combinar ambos os DataFrames para visualização\n",
    "comparison_df = pd.merge(historical_df, predicted_df, on='Timestamp', how='inner')\n",
    "\n",
    "# Passo 5: Plotar os dados\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Linha para temperaturas históricas\n",
    "plt.plot(comparison_df['Timestamp'], comparison_df['Historical'], label='Temperatura Histórica', color='blue', alpha=0.6)\n",
    "\n",
    "# Linha para previsões\n",
    "plt.plot(comparison_df['Timestamp'], comparison_df['Predicted'], label='Temperatura Prevista', color='orange', alpha=0.8)\n",
    "\n",
    "# Personalizar o gráfico\n",
    "plt.title('Comparação: Temperatura Prevista vs. Temperatura Histórica', fontsize=16)\n",
    "plt.xlabel('Tempo', fontsize=14)\n",
    "plt.ylabel('Temperatura (°C)', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
